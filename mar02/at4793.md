---
title: "Scheduling"
author: Aakriti Talwar <at4793@nyu.edu>
---
# SCHEDULING

# Introduction

# WHAT IS THEMIS ?

Themis is a New scheduling framework where ML workloads bid on available resources that are offered in an auction and allocates GPUs to winning bids.It aims to fairly apportion GPUs across workloads in shared cluster.

## WHAT IS DECIMA?

Decima is a highly efficient scheduling policy that uses reinforcement learning (RL) and neural networks tolearn workload-specific scheduling algorithms without any human instruction. It improves average job completion time by at least 21% over hand-tuned scheduling heuristics, achieving up to 2× improvement during periods of high cluster load

## WHAT IS SPARROW ?

Sparrow is a stateless distributed scheduler that adapts power of 2 choices(Po2C) load balancing techinique i.e. scheduling each task by probing two random servers and placing the task on the server with fewer queued tasks.

## TERMINOLOGIES 

## THEMIS 

1. Sharing Incentive - if there are a total N users sharinga cluster C, every user’s performance should be no worse than using a private cluster of size C/N.
2. Pareto Efficiency - Pareto efficiency is when maximum level of efficiency, and no change can be made without making someone worse off.
3. Envy Freedom - Players do not get better utility by swapping or changing their allocated resources.
4. ML App -  collection of one or more ML model training jobs,  corresponds to a user training an ML model for a high-level goal

## SPARROW

1. Wait Time - to describe the time from when a task is submitted to the scheduler until when the task begins executing
2. Service Time - to describe the time the task spends executing on a worker machine
3. Job response time - describes the timefrom when the job is submitted to the scheduler untilthe last task finishes executing.
4. Delay - to describethe total delay within a job due to both scheduling and queueing.

# Motivation

## Themis

Problem : -
- ML becoming widespread, resource intensive and ML jobs require a number of GPUs
- It is beneficial to consolidate GPU resources into a shared cluster since it saves deployment costs and promotes resource sharing.
- But there are no ML workload-specific mechanisms to share a GPU cluster in a fair manner
- Fairness is important because users require appropriate sharing incentive because otherwise either  the users would have to suffer long wait time or they would have to abandon shared clusters to move to individual clusters.

Why ML workloads are different than usual ones? :-

- long running tasks need to be gang scheduled( So DRF can violate sharing incentive)
- each task in job runs for numerous iterations
- placement sensitivity means that jobs with same aggregate resources could have widely different performance, violating sharing incentive

Hence normal scheduling algorithms can lead to unfair resource allocation for ML workloads.

## Decima

Problem : - 
- Efficient utilization of expensive compute clusters matters for enterprises: even small improvements in utilization can save millions of dollars at scale and cluster schedulers are key to this.
- Current cluster schedulers rely on heuristics that prioritize generality and forego potential performance optimizations.
- workload-specific scheduling policies that use this information require high level knowledge and effort alongside significant labour costs.
For recurring jobs, which are common in production clusters,reasonable estimates of runtimes and intermediate data sizes may be available.Most schedulers ignore this since its difficult to use them for scheduling policy.

Challenges of using job-specific information in scheduling decisions

1. Dependency-aware task scheduling - . Ensuring this requires the scheduler to understand the dependency structure and plan ahead. designing an algorithm to generate optimal schedules for
all possible DAG combinations is intractable
2. Setting the right level of parallelism - For all jobs, assigning additional parallel tasks beyond “sweet spot” in the curve adds only diminishing gains. Hence, the scheduler should reason about which job will see the largest marginal gain from extra resources and accordingly pick the sweet spot for each job.

## SPARROW 

Problem :- 
- Data Analytics frameworks can analyze large volumes of data with ever lower latency.
- Jobs composed of short , sub second tasks are difficult to schedule.
- long running batch jobs are broken into short tasks, so scheduling decisions should be taken at a very high throughput

KEY CHAllenge with decentralized design : - 
- providing response times comparable to those provided by a centralized scheduler, given that concurrently operating schedulers may make conflicting scheduling decisions.

# Approaches

## THEMIS

# Finish-Time Fair Allocation
Fair sharing concerns for ML apps: - 

A. ML Task Durations

1. median task around 3.75 hurs long , taskk for Big data analytics much shorter.
DRF provides instantaneous fairness , but can be disastrous for ML task , can lead to new arriving jobs waiting for long time
2. Least attained Service - GPUs are leased for acertain duration, and when leases expire, available GPUs are given to the job that received the least GPU time thus far , does not consider placement constraint

B. Placement Preferences

1. ML Apps have different placement preferences arising from different computation and communication needs.eg VGG prefers local task placement whereas INception does not since VGG - high parameters , incurs overhead to work over network
2. Ignoring placement affects Sharing Incentive, Pareto efficiency and envy-freedom. So there is need for new fairness metrics

# Metric: Finish-Time Fairness
     p = Tsh/Tid
     Tsh - time taken by task in shared cluster
     Tid - time taken by task independant (1/N) share if the cluster

- new wider interface between the app and the scheduling engine that can allow the app to express 
a preference for each allocation

# Mechanism: Partial Allocation Auctions 

# Why Auctions ?
Using a Straw man policy apps can submit false information about their p values so require manual monitoring. This method fails to incentivize truth-telling and violates Strategy Proofness

# Types of Auctions:- 

A. One shot Auction :-
1. Initial Allocation - Proportionally Fair Allocation by maximixing fairness matrix.ensures it is not possible to increase the allocation of an app without decreasing the allocation of at least another app
2. Incentivising Truth telling - PARTIAL ALLOCATION mechanism allocates only a fraction of Ai’s proportional fair allocation and takes remaining as hidden payment

Problem - there could be unallocated GPUs that are leftover from hidden payments. Hence violates SI

B. Multi round auctions:-  
To ensure p<= 1 for maximum apps:- 
1. Round-by-Round Auctions - 
- Allocation done only for a lease duration
- at start of each round p value is updated taking into account that work and placement preferences are time varying in ML apps

2. Round-by-Round Filtering --
- Filter out 1-f fraction of apps with maximum value of p.
- An app that lost the last auction round would have greater liklihood of winning next auction since its p vlue would have increased by next round and would have moreprobability of getting selected in 1-f fraction.

3. Leftover Allocation -
- Leftover GPUs are allocated on random basis to apps that did not participate in auction

# THEMIS Scheduler Architecture

# Need for new scheduler
1. pessimistic schedulers allow only single app visibility to resources, but for multi round auctions we require resources to be visible to many apps but allocated to only one.
2. optimistic scheduler - coupling of visibility and allocation in a lock-free manner makes it hard to realize a global policy like finish-time fairness

# Two-Level Semi-Optimistic Scheduling
Arbiter(Cross App Scheduler) -
1. Asks all apps for finish fair time matrix
2. Initiates auction, offer available to fraction of apps with worst values for p
3. Each app's Agent replies witha bid  that contains preferencefor desired resource allocations
4. Arbiter picks winning bids based on partial allocation and leftover allocation, notifies respective agents
5. AGENT propagates the allocation to the ML app scheduler

as seen in *Figure T a*

# Semi-Optimistic Concurrency Control :- 
- enforces semi-optimistic concurrency control that is visibility of resources to multiple apps but allocation to single apps

# IMPLEMENTATION

- Themis implemented on top of Apache Hadoop YARN. 
- This includs Submarine , new framework  for running ML training jobs atop YARN
- Submitted apps managed by Submarine Application Master which is modified to implement ML App Scheduler and Agent.
- Implement profiler to prepare accurate bids
- THEMIS’s ARBITER is implemented as a separate module in YARN RM.
- gRPC-based interfaces between the AGENT and the ARBITER to enable offers, bids, and final winning allocations.
-  To handle allocation changes at runtime, the programs checkpoint model parameters to HDFS every few iterations

# EXPERIMENT 
Scheduling Policies against which Themis is evaluated :-

1. Gandiva
2. Tiresis
3. Optimus
4. SLAQ
5. SRTF, SRSF

# Evaluation Metrics 
1. Finish Time fairness : a lower value of maximum value of p across apps indicates higher fairness.
2. GPU Time : If for similar conditions scheduling scheme is taking less GPU time then it is more efficient.
3. Placement Score :  inversely proportional to slowdown, S, that app experiences due to this allocation

# Macrobenchmarks
- THEMIS gives 2.2X to 3.25X better maximum p values compared to all baselines.
- THEMIS betters Gandiva by ~4.8% and outperforms SLAQ by ~250%
- THEMIS gives the best placement scores (closer to 1.0 is better).
- THEMIS is the only scheme that maintains sharing incentive even in high contention scenarios

this can be seen in *Figure T b* and *Figure T c*

# Microbenchmarks
- When the workload consists solely of network-intensive apps, THEMIS performs ~1.24 to 1.77X
better than existing baselines on max fairness
- In the workload with 100% network-intensive apps, THEMIS performs ~8.1% 
- Error Analysis - almost similar for all schemes
- Truth Telling - assuming one lying app that overreports resources, it can be seen that after a particular time lapse the hidden payment for lying app increases and it loses a big chunk of resources to other apps.

## DECIMA

# key contributions:
1. A scalable neural network design that can process DAGs of arbitrary shapes and sizes, schedule DAG stages, and set efficient parallelism levels for each job.
2. A set of RL training techniques that for the first time enabletraining a scheduler to handle unbounded stochastic job arrival sequences.
3. Decima, the first RL-based scheduler that schedules complex dataprocessing jobs and learns workload-specific scheduling policies without human input.
4. An evaluation of Decima in simulation and in a real Spark cluster,and a comparison with  scheduling heuristics.

# CHALLENGES FACED

# The DAG Scheduling Problem in Spark
- Spark job consists of a DAG whose nodes are the execution stages of the job. 
- input for each stage is output from parent stage and task can run for one stage once all tasks for parent stage are completed.
- parallelism for each job depends on number of executers

# Scheduling decisions
- deciding how many executors to give to each job; 
- deciding which stages’ tasks to run next for each job, and 
- deciding which task to run next when an executor becomes idle

# Process for Scheduling 
- Decima scheduler uses neural network.agent takes as input, the current state of the cluster and outputs a scheduling action
- For training - number of offline experiments, where decima schedules workload, observe results and gives rewards.

# Design Challenges

- large amount of dynamic data to make scheduling decisions, challenging for neural network
- The scheduler must mappotentially thousands of runnable stages to available executors.The exponentially large space of mappings 
- training for randomly arriving jobs is difficult since it can add to rewards

# How to address the design challenges ?

- Scalable state information processing - instead of feature vector , decima uses graph neural network  which embeds the state information.
- Encoding scheduling decisions as actions -
Decima invokes the scheduling agent when the set of runnable stages in any job DAG changes.Action vector contains stage v and parallelism limit lifor v.
- Stage Selection - 
utilizes a score function which is the priority of scheduling next node.
- Parallelism limit selection - 
Decima adapts a job’s parallelism each time it makes a scheduling decision for that job, and varies the parallelism as different stages in the job become runnable or finish execution

*Figure D a*

# Training

- Decima uses a policy gradient algorithm for training to learn by performing gradient descent on the neural network parameters using the rewards observed during training.

# Challenge 1 - how to train with schocastic job arrivals ?
- the agent’s initial policy is very poor Therefore, the agent cannot schedule jobs as quickly as they arrive leads to long queues.
- To avoid wasting training time terminate initial episodes early so that theagent can reset and quickly try again from an idle state

# Challenge 2: Variance caused by stochastic job arrivals
- training set needs to contain all sorts of job arrival patterns, but while learning if scheduler encounters burst of large jobs the job queue will grow large, and the agent will
incur large penalties. On the other hand, a light stream of job will incur low penalities.
- baselines are created by averaging only over episodes with the same arrival sequence. 

# IMPLEMENTATION

# Spark Integration
To integrate Decima in Spark, we made two major changes:
1. Each application’s DAG scheduler contacts Decima on startupand whenever a scheduling event occurs.
2. The Spark master contacts Decima when a new job arrives to determine how many executors to launch for it.
# Neural network Architecture 
Transformation functions are implemented using two-hiddenlayer neural networks, with 32 and 16 hidden units on each layer.
# Spark simulator
Simulator has acces to  profiling information from a real Spark cluster and captures various real world effects.

# EXPERIMENT AND EVALUATION

- Evaluation metrics:- 
1. comparison with baseline algos eg. FIFO, SJF, Fair scheduling etc.
2. BAtched arrivals - Result can be seen in *Figure D b* Decima improves the average JCT by 21% over the closest heuristic
3. Continuous arrivals -  Result can be seen in *Figure D c* Decima improves average JCT by 29%. Decima maintains alower concurrent job count than the tuned heuristic particularly during the busy period in hours 7–9, where Decima completes jobs about 2× faster

# Deep dive with Decima 
1. Learned policies , it can change scheduling decisions according to the high level objective i.e maximum cluster utilization or reducing average JCT.
2. Impact of learnning architecture - removing any one component out of decima can lead to worse results
3. Generalizing to different workloads - if workload is similar training datset , then performs well else performs bad. can be solved by training with generalised workload

## Sparrow

# Sample-Based Scheduling for Parallel Jobs

- many schedulers operate in parallel, and schedulers do not maintain any state about cluster load. To
schedule a job’s tasks, schedulers rely on instantaneousload information acquired from worker machines

# Per-task sampling 
- Sparrow’s design takes inspiration from the power of two choices load balancing technique.
- provides low expected task wait times using a stateless, randomized approach
- per-task sampling :  place each task on the least loaded of two randomly selected worker machines
- response time increases with increasing load, because schedulers have less success finding free machines on which to place tasks. At80% load, per-task sampling improves performance by over 3× compared to random placement, but still results in response times equal to over 2.6× those offered by a omniscient scheduler

# Batch sampling
- Batch sampling aggregates load information from the probes sent for all of a job’s tasks, and places the job’s m tasks on the least loaded of all the worker machines probed.
- information from the probes sent for all of a job’s tasks,and places the job’s m tasks on the least loaded of all the worker machines probed.

# Problems with sample-based scheduling

- queue length indicator can be different from the actual wait time while choosing the scheduler.
- concurrently placing task on light worker would  result in extended wait time for one of them.

# Late Binding : 
- workers do not reply immediately to probes and instead place a reservation for the task at the end of an internal work queue.When this reservation reaches the front of the queue, the worker sends an RPC to the scheduler that initiated the probe requesting a task for the corresponding job.

# Proactive Cancellation
- proactively send a cancellation RPC to all workers with outstanding probes, or it can wait for the workers to request a task and reply to those requests with a message indicating that no unlaunched tasks remain
- sending cancellation helps reduce median response time.
- Cancellation helps more when the ratio of network delay to task duration increases, so will become more important as task durations decrease, and less important as network delay decreases.

# Scheduling Policies and Constraints

1.  Handling placement constraints - 
- Per-job constraints (e.g., all tasks should be run on a worker with a GPU)
- per-task constraints,such as constraints that limit tasks to run on machines where input data is located (reduces response time)
- Sparrow cannot use batch sampling for jobswith per-task constraints, our distributed approach still provides near-optimal response times for these jobs

2.  Resource allocation policies - 
- strict priorities : Sparrow maintains one queue for each priority at each worker node. When resources become free, Sparrow responds to the reservation from the highest priority non-empty queue.
- weighted fair shares :  two users using the same worker will get
shares proportional to their weight, so by extension, two users using the same set of machines will also be assigned shares proportional to their weight.

# Analysis

Assumptions :  We assume zero network delay, aninfinitely large number of servers, and that each server runs one task at a time.

# Per Task Sampling vs Batch Sampling
-  The probability that a particular task is placed on an idle machine decreases exponentially with the number of tasks in a job, rendering per-task sampling inappropriate for scheduling parallel job.
- Batch sampling can place all of a job’s tasks on idle machines at much higher loads than per-task sampling

# multi-core machine Analysis 
Assumptions : the probability that a core is idle is independent of whether other cores
on the same machine are idle and the scheduler places at most 1 task on each machine,
even if multiple cores are idle
- Batch sampling works even better on multicore machines since it increases the likelihood of finding an idle processing unit on which to run each task.

# IMPLEMENTATION

# 1. System Components 
 *Figure S a* 
- Sparrow schedules from a distributed set of schedulers 
- Schedulers expose a service that allows frameworks to submit job scheduling requests using Thrift remote procedure calls.
- Thrift ensues multiple languages.
- A Sparrow node monitor runs on each worker, and federates resource usage on the worker
- Frontends accept job specifications and convert into parallel tasks.
- Executor processes are responsible for executing tasks, and are long-lived to avoid startup overhead 

# 2. Spark on Sparrow -
- The execution of a Spark query begins at a Sparkfrontend, which compiles a functional query definition into multiple parallel stages. Each stage is submitted as a Sparrow job

# 3. Fault Tolerence -
- Because Sparrow schedulers do not have any logically centralized state, the failure of one scheduler does not affect the operation of other schedulers
- Sparrow includes a Java client that handles failover between Sparrow schedulers
- The client sends a heartbeat message to the scheduler itis using every 100ms to ensure that the scheduler is still alive; if the scheduler has failed, the client connects tothe next scheduler in the list and triggers a callback at the application

# Experimental Evaluation

# Evaluation Metrics :
1. fine-grained tracing of the overhead that Sparrow incurs and quantify its performance in comparison with an ideal scheduler
2. we demonstrate Sparrow’s ability to handle scheduler failures
3. we evaluate Sparrow’s ability to isolate users from one another in accordance with cluster-wide scheduling policies

# Evaluation :

1. Performance on TPC-H workload :   compare Sparrow to an ideal scheduler that always places all tasks with zero wait time
- Sparrow outperforms alternate techniques and provides response times within 12% of an ideal scheduler
- reduces median query response time by 4–8×
- batch sampling with late binding provides query response times an average of 0.8× those provided by per-task sampling

2. How do task constraints affect performance? :
*Figure S b*
- because Sparrow cannot aggregate information across the tasks in a job when tasks
are constrained, delay is longer. Nonetheless, even for constrained tasks, Sparrow provides a performance improvement over per-task sampling due to its use of late
binding

3. How do scheduler failures impact job response time?:
*Figure S c*
- When the Sparrow scheduler on node 1 fails, it takes 100ms for the Sparrow client to detect the failure, less than 5ms to for the Sparrow client to connect to the scheduler on node 2, and less than 15ms for Spark to relaunch all outstanding tasks.

4. How does Sparrow compare to Spark’s native, centralized scheduler? :
- Spark’s existing centralized scheduler cannot provide high enough throughput to support sub-second tasks
- Although throughput can be increased with improved engineering.

5. How well can Sparrow’s distributed fairness enforcement maintain fair shares?:

- Sparrow’s fairness mechanism lacks any central authority with a complete view of how many
tasks each user is running, leading to imperfect fairness.
- But it can quickly  allocates enough resources as user demand changes.

6. How much can low priority users hurt response times for high priority users? :
-  While Sparrow prevents the high priority user from experiencing infinite queueing delay, the high priority user still experiences 40% worse response times when sharing with a demanding low priority user than when running alone on the cluster

7. How sensitive is Sparrow to the probe ratio? :
- Changing the probe ratio affects Sparrow’s performance most at high cluster load.
- there is s a sweet spot in the proberatio: a low probe ratio negatively impacts performance because schedulers do not oversample enough to find lightly loaded machines, but additional oversampling
eventually hurts performance due to increased messaging

8. Handling task heterogeneity ? :
-  Sparrow does not perform as well under extreme task heterogeneity: if some workers are running long  tasks, Sparrow schedulers are less likely to find idle machines on which to run tasks

# Trade-Offs

## THEMIS
- THEMIS is better than other schemes on finish-time fairness while also offering better cluster efficiency
- benefits improve with increasing contention and fraction of placement sensitive apps
- enables a trade-off between finish-time fairness in the long-term and placement efficiency in the short-term
- f = 0.8 and lease time of 10 mins give the best  results

## Decima

- Decima’s action specifies job-level parallelism , as opposed fine-grained stage-level parallelism. This design choice trades off granularity of control for a model that is easier to train. In particular, restricting Decima to job-level parallelism control reduces the space of scheduling policies that it must explore and optimize over during training

- Decima trades off memory fragmentation against clearing the job queue more quickly.This trade-off makes sense because small jobs (i) contribute more to the average JCT objective, and (ii) only fragment resources for a short time. By contrast, Tetris greedily packs tasks into the bestfitting executor class and achieves the lowest memory fragmentation.Decima’s fragmentation is within 4%–13% of Tetris’s, but Decima’saverage JCT is 52% lower, as it learns to balance the trade-off well.

## Sparrow 

- Sparrow  makes approximations when scheduling and trades off many of the complex features supported
by centralized schedulers in order to provide higher scheduling throughput and lower latency. In particular, Sparrow does not allow certain types of placement constraints does not perform bin packing, and does not support gang scheduling.
- Sparrow enforces strict priorities or weighted fair shares when aggregate demand exceeds capacity. 
- Sparrow emphasizes constraints over job placement, such as per-task constraints (e.g. each task needs to be co-resident with input data) and per-job constraints (e.g., all tasks must be placed on machines with GPUs)
- The downside of late binding is that workers are idle while they are sending an RPC to request a new task from a scheduler.So schedulers wait to assign tasks until a worker signals that it has enough free resources to launch the task. Leads to a 2% efficiency loss but in environments where network latencies and task runtimes are comparable, late binding will not present a worthwhile tradeoff.
- Strict priorities policy in sparrow trades of simplicity for accuracy: nodes need not use complex gossip protocols to exchange information about jobs that are waitingto be scheduled, but low priority jobs may run before high priority jobs if a probe for a low priority job arrives at a node where no high priority jobs happen to be queued



# Open Questions and Future Work

## Themis

Open Questions related to Themis :- 

1. Themis chooses to define “fair” itself. Is there any other measure of fairness that we might want to use?
2. Are there any drawbacks since it requires a widening of the API between the apps and the scheduler.
3. The research paper does not mention about fault tolerence and fault tolerence for Themis can be included in its future work.

## Decima 

1. Robustness and generalization - 
  - Decima can learn generalizable scheduling policies that work well on an unseen workload. However, more drastic workload changes than interarrival time shifts could occur. To increase robustness of a scheduling policy against such changes, it may be helpful to train the agent on worst-case situations or adversarial workloads
  - adjust the scheduling policy online as the workload changes.The key challenge with an online approach is to reduce the large sample complexity of model-free RL when the workload changes quickly.
   “meta” scheduling agent that is designed to adapt to a specific workload with only a few observations
2. Other learning objectives - 
- Till now evaluated Decima on metrics related to job duration (e.g., average JCT, makespan).
Shaping the reward signal differently can make Decima to meet other objectives, too
3. Preemptive scheduling - future work might investigate more fine-grained and reactive preemption
in an RL-driven scheduler such as Decima.
4. Potential networking and system applications -
 - the scalable representation of input DAGs has applications in problems over graphs,
such as database query optimization and hardware device placement. The variance reduction technique generally applies to systems with stochastic, unpredictable inputs

## Sparrow

- Sparrow does not currently support preemption when a high priority task arrives at a machine running a lower priority task
- Current design does not handle interjob constraints (e.g. “the tasks for job A must not run on
racks with tasks for job B”). Supporting inter-job constraints across frontends is difficult to do without significantly altering Sparrow’s design
- Some applications require GANG SCHEDULING, a feature not implemented by Sparrow. While Sparrow often places all jobs on entirely idle machines, this is not guaranteed, and deadlocks between multiple jobs that require gang scheduling may occur
- Handling worker failures is complicated by Sparrow’s distributed design. A centralized state can be used that relies on occasional heartbeats to maintain a list of currently alive workers
-  Sparrow could potentially improve performance by dynamically adapting the probe ratio based on cluster load





