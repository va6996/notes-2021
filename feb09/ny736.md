
|title | author|
|---- | ----|
|"The Data Center as a Computer Ch1 and 2"| Neeraj Yadav <ny736@nyu.edu>|

# Introduction
The widespread use of internet based services like search, web-based email, online maps, social networks and video streaming and the availablity of high speed connectivity have shifted a trend towards  server-side or cloud computing. The scale of these applications make it unfeasible to execute them on a single machine or a rack. Hence we need warehouse-scale computers (WSCs). WSCs have software infrastructure, data repositories and hardware platform at massive scale.

# Motivation
The computation at WSCs for some application may incur a low cost per user since the resources are shared and utilized better. Server and storage is easier to maintain at WSCs.  It improves user experience since they don't need to take backups and get ubiquitous access. It also allows software vendors to update and fix the bugs more easily since they just need to update WSCs instead of millions of users. However, running an application on a cluster of thousands of machines is challenging. WSCs is designed to be fault tolerant and to achieve optimal throughput, latency, cost efficiency and availability.

# Difference between Data Centers and WSCs
"Data centers are buildings where multiple servers and communication gear are co-located  because  of  their  common  environmental  requirements  and  physical  security  needs,  and  for  ease of maintenance." As per this definition we can call WSCs a type of data centers. However there are minute differences. Data centers host a large number of medium or small sized application whereas WSCs host a small number of large sized applications. Data center have dedicated hardware for different computing requirements while WSCs use a relatively homogeneous hardware, software platform and the common resource management layer which facilitates the deployment flexibility. 

# Architecture : WSCs
1. **Servers** : Servers are hardware builiding blocks of WSCs, mounted within a rack and interconnected through local ethernet switches. These local rack level switches have multiple uplink connections with cluster level ethernet switches.  

1. **Storage**: Disks and SSDs are the building blocks of WSC storage system. Disks and SSDs can be connected directly to servers (DAS) or disaggregated as part of Network Attached Storage (NAS). DAS reduces hardware costs and improves network utilization while NAS provides higher QoS and eases deployment. 

1. **Networking Fabric**: Network switches with large number of ports are fast but expensive. While commodity rack switches are cheap but slow.

1. **Building and Infrastructure**: Infrastructure is crucial for WSCs availability. WSCs generate lot of heat, proper cooling should be there for efficient functioning.

1. **Power Usage**: Energy and power usage impact a lot on WSCs cost. Hence a lot of work is done to optimize the power usage. CPU is the dominant power consumer in WSCs.

1. **Handling Failures and Repairs**: The massive scale of WSCs make it prone to fault and failures. WSCs are optimized for automatic and fast recovery.

# Software Layers in WSC Deployment

### Platform Level Software
The common firmware, kernel, operating system distribution, and libraries expected to be present in all individual servers to abstract the hardware of a single machine and provide a basic machine abstraction layer. This is very much similar to what we expect in a regular enterprise server platform. Virtualisation is popular in WSC for IaaS (Infrastructure-as-a-Service). Virtual machine provides an interface to manage security and performance isolation of application. Containers are an alternate abstraction that allow for isolation across multiple workloads on a single OS instance. They are lightweight, smaller in size and much faster to start as compared to VM.

### Cluster-Level Infrastructure Software
Cluster level infrastructure layer manages resources and provides basic services to WSCs silmilar to what an operating system provide for a single computer. It constitutes of three groups of software infrastructure:
1. **Resource Management**: It's an interface to allocate group of machines to a user job. This can be automated using scheduling algorithms. The scheduling algorithms should consider fault tolerance, power consumption and cooling requirements while allocationg machines. For eg. Kubernetes is used to manage containers.

1. **Cluster Infrastructure**: This includes having basic functionalities for a large scale distributed applications like reliable distributed storage, remote procedure calls (RPCs), message passing, and cluster-level synchronization. 

1. **Application Framework**: Its purpose is to make the programmer's life easy by hiding the inherent complexities of a large scale system.  For eg. MapReduce has software infrastructure to automatically handle data partitioning, distribution, and fault tolerance. 

### Application Level Software
Software that implements a specific service. It is of two kinds, online services like Google Maps, Gmail, Google Search and offline computations like processing satellite images. The breadth of services has resulted in diverse application requirement. For eg. a Web Search may relax the consistency while Click on ads must be consistent. However there is a tradeoff between making specialized hardware for applications. A specialized hardware may give good performance in some cases but generally the programmers are changing the code of application regulary which favors the idea of general-purpose design.

### Monitoring Infrastructure
The monitoring information must be very fresh so that corrective actions can be taken quickly. Important information like throughput and latency of user's request can be collected from front-end servers. Debugging tools are used to know why a service is slow. They are divided into two categories: black-box monitoring systems and application/middleware instrumentation systems. 

# Trade-offs

### Desktop vs WSC
WSCs has a homogenous computing platform, data-parallelism and request-level parallelism improves the performance of applications on WSCs while frequent hardware failure and workload churn hinders the performance of applications on WSCs. A desktop can be considered as a fault-free hardware operation for months or years and is more reliable compared to WSCs.

### Buy vs Build
Traditional IT infrastructure widely use third-party software components like databases and system management software, and run their application on top of the purchased components while if we write both application-specific logic and the cluster-level infrastructure software by ourself then we are adding flexibility to the system and making it cost effective. Building the cluster-level infrastructure helps in better optimization and perfromance bugs can be handled quickly.

### Replication
Replication improves both throughput and availability. However, if replicas are modified often then it becomes hard to maintain the consistency over all the copies/replicas.



|title | author|
|---- | ----|
|"The Google File System"| Neeraj Yadav <ny736@nyu.edu>|
# Introduction
The paper aims at designing a distributed file system which is scalable, reliable, available, fault tolerant and efficient for sequential read and writes. The paper presents GFS which provides concurrent access to hundreds of clients to files using a single master and thousands of chunkservers which stores files.


# Motivation
As the data grew there was a need to split the data across many machines for efficient processing. Google developed a distributed File system called as GFS that can handle the large data and files. GFS was developed to work reliably on commodity hardwares (cheap machines) where component failure is very common. It was designed to store the big files from 100Mbs to multiple Gbs. Though it can work on small files but it was optimized for big files. Because Google needed an infrasturcture for the big files and GFS was exclusively developed for the internal use. It was developed for big sequential reads and writes (appending at the end of file). Because Google observed that their applications which would run on top of GFS would be reading the data sequentially and also there would be negligible random writes and overwriting. The focus was on high sustained bandwidth (throughput) than low latency. Google observed that they don't need a strong consistent system and it is okay to miss one or two search results among thousands. Hence the system relaxed the consistency. 


# Approach
A GFS cluster has multiples clients, one master and multiple chunkservers. Chunkservers store the actual data as Linux files. Files are divided into fixed size (64 Mb) chunks. Chunks are labelled by 64-bit unique global ids (chunk handles). A chunk is by default replicated over 3 chunkservers to attain reliabality. A user can also specify the number of replicas. Master keeps track of which chunks belongs to which files and helps the client to locate the chunks. Master contains a table with keys as file name and values as list of chunk handles. For example, a 1 GB file would be divided into 17 chunks (need to store ids/handle of these 17 chunks). It also contains a table with chunk handles as keys and list of chunkservers holding the corresponding data as values. Since a chunk is replicated over many chunkservers, Master needs to store the mapping from chunk handle to all those chunkservers. This table helps to locate all the replicas of a given chunk handle. The Master contains a version number of each chunk. It also contains info whether a chunk is primary and the lease expiration time. 

Master contains all the metadata in RAM for fast access. However to prevent the data loss on reboot, Master saves the metadata to hard drive (non-volatile memory) except the version number, lease experiment time and table from chunk handle to chunkservers. Because a chunkserver might be dead when master rebooted. Hence, master would not want to send the location of dead chunkservers to clients. Every time master reboots, it would check the health of chunkserver by sending hearbeat messages and reconstructs the mapping. Apart from saving the logs on the disk, Master also create checkpoints in a regular interval of time for the fast reloading afer a crash.

![GFS Cluster](https://github.com/neeraj71/BDML-/blob/main/gfsim.png)

### Read
* Application specifies the file name and offset.
* Client computes the chunk index (divide offset by 64). Client sends read request to Master containing filename and chunk index.
* Master reverts back with chunk handles and location of replicas.
* Client caches (only metadata) this to reduce the interactions with Master (don't want to overload the Master).
* Client talks to one of the closest replicas. Client tells the chunk handle and the byte range to read to chunkserver.
* Chunkserver reads the desired bytes and returns the data to clients.

### Record Appends
* Application originates record append request.
* Client aks for location of Primary (lease holder) chunkserver and all other replicas (Secondary) to Master. If there exists a chunkserver with lease or Primary chunkserver  then the master returns the locations of Primary and Secondary chunkservers. Otherwise the Master finds out set of chunserver that have most up-to-date replicas.
As we know, Master holds the version number to distinguish between the stale copies of the chunk vs up-to-date. A replicas is said to be most up-to-date if its version number is equal to the version number that Master knows. Master picks one of the most up-to-date chunkservers as the Primary chunkserver and other most up-to-date chunkservers as Secondary. Master grants a lease for 60sec to Primary chunkserver. Lease mechanism is used for consistent mutation across replicas. This is ensured by forcing a single Primary for a chunk. Master sends heartbeat messages to Primary but if it doesn't receive any reply from Primary it won't assign a new Primary until the lease expires. If there was no lease mechanism, and Master doesn't receive reply from Primary due to network fault then the Master would have assigned a new primary resulting into two primary chunkservers writing over the same chunk (*Split Brain*) and the system wouldn't be consistent. 
* Client sends a copy of the data to be appended to the primary and secondary chunkservers. Primary and secondary chunkservers write that data to temporary location (not appended yet). Once the replicas responds that they have received the data. The client sends append request to primary. (Paper suggests that in order to maximize bandwidth and throughput, the client would send data to only one of the replica, then replicas send the data in a chain to all other replicas)
* Primary receives requests concurrently from many clients. It picks some order and executes the requests one at a time. Primary decides offset and writes record at the end of the chunk and asks secondary to write at the same offset in the same order.
* However Secondary may or may not be able to append the record. If Secondary chunkservers were successful in appending the record, they reply "yes" to primary and primary says success to the client. If even one of secondary chunkserver fails to append the record, the primary would report failure to the client. So, if client receives success it implies that the record is appended at the same offset in all replicas. If client receives failure then the atleast in one of the replicas the record wasn't appended. In such cases, the primary would try to refresh the lease and start the process again. However, if the primary is unable to do so, the client would make a fresh request to Master. The client would keep making requests until the record is appended. This ensures that a record would be appended atleast once in all the replicas.

# Trade-Offs
### Relaxed Consistency
GFS has a very good throuhput but at the cost of some relaxation in consistency during writes/record appends. Suppose that client A made an append request to the primary chunkserver (with 2 secondary chunkservers). The client got a success reply from the primary. This means that record was appended in all three replicas. Now, the client B makes an append request and got a failure reply from the primary because one of the secondary chunkserver failed to append the record. However primary and the other secondary successfully appended the record. Client B would continue to retry until it gets success. In between client C makes an append request to primary and gets success reply. Now  client B again makes an append request to primary and this time gets a success reply. This results in the duplication of the record of client B at the primary and one of the secondary chunkserver. Moreover the order in which records got appended is not same in the two secondary chunkservers. Also, if a client D makes an append request and it gets failed and the client dies then atleast one of the replica would be missing the record forever. However, these inconsistencies occur very rarely and doesn't matter much in application which uses GFS, for eg. in search results if we miss out a single record among thousands.

### Chunk Size
GFS used a chunk size of 64Mb which is quite large. Larger chunk size reduced the client master interaction because reads and writes on the same chunk require only one request from client to master and after that client can used cached locations. This is supported by the fact that most of read/write operation are sequential. Also, large chunk size implies fewer chunks and less master's metadata in RAM. It also reduces the network overhead. The disadvantage of large chunk size is that it can lead to fragmentation if there are large number of small files. If file is small then it would get fitted in one chunk and if multiple clients are accessing that chunk then it would become hotspot.

### Replica Location
In GFS the replicas are spread across the racks. This give more reliability if an entire rack is damaged. Since we choose one of the replicas during read, this would give us the average bandwidth of multiple racks while reading. The downside is that during write operations the data has to flow through multiple racks.  

### Client Caching: Throughput vs Consistency
Client caching improves throughput and perfromance but there are consistency issues. As clients caches the chunk locations, it's possible that they may read from stale replica before the information is updated. This results to inconsistency but fortunately it is short lived. Because the cached metadata expires after a fixed time or if the file is reopened again.

### Garbage  Collection

GFS doesn't reclaim the space immediately during deleting a file. It first renames that file and removes them only if the file was renamed x (default value 3) or more days before. The advantage of this process is that it is done when master is relatively free and it is simple and reliable. The disadvantage is that we won't be able to access the storage immediately. 

# Open Questions and Future Work
GFS proved to be very successful for Google as many applications like BigTable, MapReduce run over it. However as number of files increases, the metadata also increases making it challenging for master to store the metadata in RAM. This is an open problem to store the metadata for large number of files. Other areas that need to look after are to manage the load on single master from thousands of clients and automatic recovery of master (in case of failure) without human intervention. In future, a two-phase mechanism can be tried out to make the system consistent while appending the records. For this, the primary chunkserver first needs to ask to all the secondary chunkservers whether they can append the request and if they reply "yes" then only it orders them to append the record. A duplicate detection mechanism can be used to remove the duplicate appends in the replicas.



|title | author |
|---- | ----|
| "MapReduce:Simplified Data Processing on Large Clusters" | Neeraj Yadav <ny736@nyu.edu>|

# Introduction
The paper aims at designing an abstraction that can facilitate the distributed compuatation but hides the complications like parallelization, fault tolerance and load balancing. The paper presents a programming model MapReduce inspired by the map and reduce primitives in the Lisp to do large scale compuatations. The user defines a map function that processes the *key/value* pairs to produce intermediate *key/value* pairs and a reduce function to merge the intermediate values with the same intermediate key.

# Motivation
Suppose that we want to sort 1 TB of data on a single computer. Single computer reads at 30MBps from disk. So, it would take 33000 sec or 10 hours just to read the data. Hence we need to move towards the distributed computation (using thousands of CPUs). However, the distributed computation presents many challenges like coordination among nodes, recovery from node failure, sending data to/fro from nodes, optimizing for locality and debugging. MapReduce solves this by providing automatic parallelization, load balancing, locality optimization and handling of machine failures.

# Approach

MapReduce is a programming model which takes *key/values* pairs as input and produces a set of *key/values* pairs as outputs. The computation is done using two functions defined by the user:
1. **Map** : Process input *key/value* pairs and generates the intermediate *key/value* pairs. Intermediate values with the same key is then grouped together and passed to the Reduce Function. 

1. **Reduce** : Merges all values corresponding to the same intermediate key.

### Word Count Example

The map function emits each word and its occurences ("1" here). The reduce function sums all the occurences for a word.

```
map (String key, String value):
  // key: document name
  //value : document contents
  for each word w in value:
    EmitIntermediate(w, "1");
 
reduce(String key, Iterator values):
  //key: a word
  // vlaues: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```

![MapReduce Architecture](https://github.com/neeraj71/BDML-/blob/main/mapredimage.png)
### Execution

MapReduce works on top of GFS. So, the basic infrastructure of dividing the files is same as we saw in GFS. We have a master node and thousand of chunservers (workers) where the actual computation is performed. Master stores the metadata: state (*idle/in-progress/completed*) and the identity of worker machine (*non-idle* tasks). Master's function is to pass the location of intermediate data produced by map tasks to reduce tasks.

* User program calls the MapReduce function. Input files are divided into chunks of size 16MB to 64 MB (controlled by user). 
* Master node picks the idle workers (chunkservers) and assigns map and reduce task.
* Worker with map task read the content from chunk and apply the map function to the input. Intermediate results are buffered in the memory and periodically written to the disk.
* Location of these intermediate results is passed to the master which passes it on to reduce workers. 
* Reduce Workers read the intermediate results and sorts it by the intermediate keys to group the same keys together.
* Reduce workers iterate over the sorted intermediate results and for each unique intermediate key, passes the key and values to the Reduce function defined by user. 
* Each Reduce task yields one output file, we get R output files in the end. After the completion of map and reduce tasks control is returned back to user program.

### Locality
Since, the data is stored using GFS. We know that files are divided into chunks and are stored at different chunkservers (workers). MapReduce takes the advantage of this fact and all the 'map' and 'reduce' computations are done at the chunkservers (workers) itself.  In general, the master schedules the map task to a machine which is closest to the machine containing the data. This minimises the transfer of data and helps in sustaining the network bandwidth.

### Combiner Function

Combiner function is used if there is significant repetition in the intermediate keys and the reduce function is commutative and associative. The combiner function does partial merging of the intermediate data and its code is similar to the reduce function. It is executed on the same machine that performed the map task. Hence it reduces the amount of data to be sent over the network for the reduce tasks.

# Trade-Offs

### Redundant Computations for fast performance: Computation Resources vs Time
Stragglers are the machines that take an unusally long time to complete one of the last few map or reduce tasks. This increases the execution time. To counter this, whenever a MapReduce operation is close to completion the master schedules backup tasks of the in-progress tasks. If any one of the backup task is completed then the task is marked as completed. This leads to redundant computation but reduces the execution time. 

### Task Granuality
The map phase is divided into M tasks and the reduce phase into R tasks. Having M and R much larger than the number of worker machines gives worker machines an opportunity to perform many tasks. This improves load balancing and speeds up failure recovery. However, we can't make M and R arbitrarily large since the master has to make O(M+R) scheduling decisions and store O(M*R) states in memory.

# Open Questions and Future Work

Since MapReduce works on top of GFS. To optimize the MapReduce operation we need to deal with all the problems that we discussed for GFS like the single master, the RAM of master can't be arbitrarily large and consistency issues. In future, work need to be done to devise an optimal algorithm to more efficiently schedule the backup tasks. This would lead solve the problem caused by the stragglers machines and would improve the execution time.   
Since single Namenode leads to unreliablity. Also, recovering a Namenode require human interventions and may take several minutes. Hence HDFS federation uses several independent Namenodes (no inter-coordinations). All the datanodes are linked to all the Namenodes in the cluster. Multiple Namenodes provide horizontal scalability and allow multiple users to run their application on different Namenodes. 

