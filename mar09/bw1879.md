---
title: "Topic You are Writing About"
author: Brad Windsor <bwindsor22@gmail.com>
---
# Introduction
The papers this week cover 3 general topics:
* Optane Persistent Memory Module -- Memory occupying a tier between SSD and RAM
* Persimmon -- a persistent memory system for in-memory, crash-consistent memory
* SplitFS -- a file system for persistent memory

# Optane Persistent Memory Module
https://arxiv.org/abs/1903.05714

## Introduction
This document is an academic study of an Intel technology. It is a new type of memory which enables faster R/W speeds at higher volumes. It goes into detail on experiments for the new RW speeds.

## Motivation
Memory can be thought of as a sliding scale from cheap, large, and slow (Tape) to fast, small, and expensive (Ram).  Optane DC memory sits at a tier just below Ram and above SSD drives; it is faster than a SSD drive, but larger than RAM. The module can read/write at 6.6/2.3 GB/s, but in the experimental format has 3TB of memory. 

For a very small example, this means the setup could iterate through a 3TB dataset when training a model, without having to read every datapoint from disk, meaning a much faster training.

The module also has the benefit that memory can be perisistent in App Direct mode, which means it can be used to store frequently used data, vs RAM which is not persistent. 

## Approaches

* Optane DC PMMs currently come in three capacities: 128 GB, 256 GB, and 512 GB.

* Optane DC PMMs can operate in two modes: Memory and App Direct modes.
    * Memory mode uses Optane DC to expand main memory capacity without persistence. Usually this is paired with a DRAM cache for speed. 
    * App Direct mode is useful for building storage systems out of Optane DC. There's no DRAM cache here, and the device is typically managed via a file system.

* The authors look at a number of experimental settings to compare to DRAM:
    * Latency is roughly 4x slower than DRAM.
    * R/W is roughly 8x slower than DRAM. This comparison exists as the number of threads doing R/W increases for both DRAM and Optane.
    * When looking at integer/floating point workloads per a set of benchmarks (SPEC 2017), Optim is equivalent for the integer workload, and 15% slower for the floating point one. This experiment showed how important running in memory mode is; the result is 60% slower without it.

* Authors also explore using database software (SQLite, MongoDB, MySQL, etc.) to find how much its use as a file system improves performance. They routinely see speedups of 10-30x across database types, though the degree of speedup depends on the customization done for the setup, customizations like rewriting the application to use memory-mapped Optane.

## Trade-Offs
There are two clear sets of tradeoffs. The first is tradeoffs vs other types of memory; you wouldn't want to use Optane to store your movie collection (high size) or run the braking system of your autonomous vehicle (high speed). 

The second is tradeoffs in the setup. The memory mode is faster but may not be sufficient for stateful applications. With other applications, the configuration might not be worth the expense saved from higher RAM; the authors are using a special Redis fork with specific memory libraries.


## Open Questions and Future Work
Two obvious questions arise, which are (1) how much can RAM memory volume keep increasing, and (2) what possible improvements are there to speed up SSD drives? The second might not be as relevant because of the Optane memory's physical proximity, but if RAM can grow sufficiently this technology might not be needed.

# Persimmon 

## Introduction
This paper shows how to build a faster memory system by combining DRAM and persistent memory.

## Motivation 
DRAM is the fastest form of memory, but if the computer crashes, the contents of this memory becomes unavailable. Thus it makes little sense to run a database system entirely in DRAM; the system would lose all information if the host ever failed. Persistent memory is a slightly slower form of memory, with some additional overahead and complication to ensure the memory is crash-consistent. This memory is slower, but it can also be more complicated to implement, as all operations must be failure-atomic so that they either fully succeed or fail. This means doing things like flushing the CPU at specific times, often a full application rewrite.

Persimmon is a fairly simple way of using persistent memory, such that all operations are still persisted, but the approach takes advantage of DRAM's higher speed and requires a faily simple application rewrite.


## Approaches
The Persimmon runtime maintains two state machines, one in DRAM, and a shadow one in the persistent memory. The persistent memory state machine exactly copies the DRAM one; the application writes to DRAM, then adds the entry to an operation log, then asynchronously performs the same action in the shadow state machine. If the log is full, meaning the shadow machine is too far behind, the DRAM thread pauses for the shadow to catch up. 

Recovery is fairly simple, in essense the shadow state machine is retrieved from the persistent memory, any remaining log operations are processed, then the state machine is copied into DRAM.

This enables a very simple API with just an initialization, read-only, and read-write operations. The authors use this to make a port of Redis and of Tapir and measure the improvement on these database systems, finding a negligible latency overhead and 5-8% throughput overhead.


## Tradeoffs 

Persimmon offers a fairly quick API for rewriting an application to use persistent memory, but this still leaves some tradeoffs. First, the application may not be as fast as a from-scratch rewrite. Second, the rewrite must organize the writes as a state machine. The authors found the rewrite of Redis difficult because Redis was not written in an OO language, but rather C for speed. As Persimmon is primarily targeting speed, this might be a common difficulty. 

However, there are still great advantages to speed and using a high-level API. One is that because all of the interaction with persistent memory is relegated to Persimmon, there's relatively less room for error in a rewrite, as the number of code changes becomes far simpler.


# SplitFS

## Introduction
With persistent memory (PM) as a new memory form, there is room for research as to how best to configure PM in a file system. SplitFS is one such file system; it operates by doing data operations in the user space with a kernel PM file system that handles metadata.


## Motivation
The goal of SplitFS's design is to accelerate common-case data operations, while continuing to pay the computational cost for metadata operations. This provides a net lower software overhead, while maintaining strong consistency. 

## Approaches

SplitFS is a combination of a user-space library file system and an existing system for accessing PM, ext DAX, with a small patch added. 

One benefit is that because SplitFS extends DAX, there's no need to rewrite the application in any way, and SplitFS benefits from DAX's well-tested code base.

A drawback is that because SplitFS routes all metadata to PM, it has a high software IO overhead for metadata operations. 

In addition to the split between metadata in PM and data in user space, SplitFS has several other distinguishing features. A collection of mmap calls ensures the file system can get data from multiple physical files at once. It uses temporary files for appends and atomic data operations. And it introduces a primitive called relink to move PM blocks from the staging file to the target file.


## Tradeoffs

SplitFS is easy to use, increases file system speed for data-intensive operations like databases, and does not require the application to be modified in any way. One tradeoff, however, is that because all metadata is written to PM, it will perform more slowly when doing metadata-intensive operations like git.
