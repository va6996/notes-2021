---
title: "Networking in Multi-tenant Datacenters"
author: Vinayak Agarwal <va2083@nyu.edu>
---
# Introduction
As multi-tenant datacenters becomes more mainstream, datacenter operators try optimizing hardware and software at the network level to remove bottlenecks or offload tasks off the CPU to NICs/switches or other components. At the same time optimization needs to made so that general datacenters can adopt a wider range of workloads traditionally restricted to specialized datacenters.

# Motivation
As more workloads move to datacenters, the datacenters need to support the network configurations previously used by these workloads to ensure a smooth transition to general datacenters. Just like we have VMs for physical machines, we need a similar mechanism for networks too.

Various generations of hardware optimizations such as RDMA have been seen, each with their drawbacks. Datacenters are now equipped with even faster networks making switch to computer memory times the bottleneck. Operators need to come up with clever mechanisms to optimize hardware for datacenters to reduce the time network operations take, building over common case scenarios. Below we see mechanisms built to make networking easier and faster in datacenters.

# Approaches
## Network Virtualization
Different workloads require different network topologies, and it becomes difficult to support them in a multi-tenant datacenter. The organizations have to build multiple networks for each of the requirement.
VMs work in the same address space as the physical network. This makes moving around VMs difficult due to the network configurations.
This calls for need for a virtualized network that supports arbitrary topologies and addressing architectures on the same network. Just like application running on a VM interacts with the VM as if it was a physical machine, the application should be able to interact with the virtual network as if it was interacting with a physical network and let the virtual network translate the actions dynamically into the physical layer.

### Requirements of Network Hypervisor
Provide packet abstraction so that the L2 and L3 service semantics are similar to that of the physical network, so that no changes need to be assumed for the endpoints from the VM. Need to provide compliance with standard TCP/IP running at the endpoints inside the VMs.
Provide control abstraction so that tenants can manage their own logical network. There is no fixed standard for the control plane. However, at the low level, there is always a datapath involved. There is a packet processing pipeline, a set of flow tables programmed by the control plane. This forms the basis of control abstraction. The end goal is to create a logical datapath that contains a set of logical flow tables so that tenant control plane can control the logical datapath with logical flow entries that actually implement the forwarding behaviour within the virtual network.

### Datapath
When a packet traverses the network, it traverses a sequence of table pipeline/datapaths. The previous datapath makes the forwarding decision for which datapath to traverse next. This datapaths are usually routers and switches. Hence, to provide arbitrary logical networks to tenants, we need to provide this virtual datapath and the ability to interconnect multiple datapaths. This allows the tenant to form arbitrary network topologies as per requirement.

### Implementation
Interconnects VMs using IP tunneling using virtual switches on the VMs.
These virtual switches are programmed by the controller cluster.
Controller cluster also runs the tenant specific control plane which program the logical datapaths provided by network hypervisor.
Only IP connectivity is requested from the physical network.
The source hypervisor tunnels the packet directly to the destination hypervisor. There is no hypervisor processing the packet in the path.

### Virtual switches
Virtual switch on the hypervisor responsible to implement complete logical network, datapatha and flow processing.
Need to execute all logical forwarding decisions before we know the logical destination port to figure out the destination hypervisor the packet will be tunneled to.
Packet processing is implemented as a sequence of flow lookups.
The first hop identifies the logical ingress port, then feeds the packet to logical datapath, which is a sequence of flow lookups. This datapath comes up with a logical forwarding decision which defines the next logical datapath. This process repeats until the final datapath comes up with logical destination port. After this the destination hypervisor is determined and the packet is tunneled.

### Controllers
Controllers responsible for calculating all forwarding states and pushing them to virtual switches.
The controller cluster needs to keep recalculating the state after each resource is moved/added/removed.
If connection dies in between updates, dataplane may operate over incomplete state. Hence, incremental updates are pushed. Applied atomically in batches. This way connection failure does not result in incomplete state.

### Encapsulation header
Certain topologies may be deployed with little address information, e.g., firewalls with exact addresses.
Network hypervisor adds tags that are visible only to the logical elements by adding additional information in the header. 
These out-of-band header fields work while being compliant with regular TCP/IP stacks.

## 1RMA
RDMA is an attractive option for modern datacenter applications due to low latency and high throughput benefits.
However, it has various drawbacks:
- Connection orientedness leading to poor scalability and failure semantics.
- Not amenable to support security key management operations.
- Not amenable to rapid iteration due to key algorithms being baked into hardware.

### Approach
Division of labor between hardware and software leading to simple and fixed function 1RMA NIC aided by 1RMA software.
Fixed function NICs with explicitly allocated resources.
- Connection free independent ops
- Explicitly finite hardware resources managed by software
- Solicitation
Implements software driven, hardware assisted congestion control algorithm.
Connection free security protocol and first-class support for security management ops.

### Connection free independent ops
Acts on fixed size ops and treats them independently. This provides fail fast behavior where the NIC tries to complete the op within a fixed time. If successful, it provides delay measures or else returns op failure notifications and reasons.
Leaves retry, ordering, congestion control and segmentation to software. 
State doesn't grow with endpoint pairs.

### Explicitly finite hardware resources managed by software
Fixed size RRT and CST maintained in NIC SRAM. Simplifies complexities associated with cache oriented RNIC designs leading to simpler hardware and predictable performance.
Having explicitly finite hardware resources enables 1RMA to manage them in software and allocate resources based on priorities.

### Solicitation
Ops not initiated unless it is assured to land its subsequent response in the NIC SRAM called solicitation window.
Once op is written to CST, it is enqueued in the Op FIFO and waits for capacity in the solicitation window. Once capacity is reserved, it is signed and dispatched.
Solicitation window based on BDP, thus limiting the severity of incasts by bounding incoming bytes.

### Software driven congestion control
For successful ops, 1RMA provides fine grained hardware delay measures as issue delay and total delay.
Issue delay - How long it took op to enter service.
Total delay - How long it took to execute the op.
Upon failure, tells why the op failed.
Uses signals from hardware to implement congestion control in software.
Reacts separately to local and remote congestion by maintaining congestion window for each remote destination direction pair and single local congestion window.

### First class support for security ops
Offers Rekey op that provides support for encryption key rotation.
Remote key rotation service can directly change the encryption key without placing trust on local software stack or involving local CPU.
Unavailability period reduces to a single RTT to rekey.

## eRPC
eRPC (efficient RPC) is a new general-purpose remote procedure call (RPC) library that offers performance comparable to specialized systems, while running on commodity CPUs in traditional datacenter networks based on either lossy Ethernet or lossless fabrics.

Optimizes for the common case and also avoids triggering packet loss due to switch buffer overflows for common traffic patterns.
*    Small messages
*    Short duration RPC handlers
*    Congestion-free networks
Optimizations for the common case boost performance by up to 66% in total. On this base eRPC also enables zero-copy transmissions and a design that scales while retaining a constant NIC memory footprint.

RPCs are asynchronous and execute at most once. Servers register request handler functions with unique request types, and clients include the request types when issuing requests. Clients receive a continuation callback on RPC completion. Messages are stored in opaque DMA-capable buffers provided by eRPC, called msg-bufs. Each RPC endpoint (one per end user thread) has an RX and TX queue for packet I/O, an event loop, and several sessions.

eRPC supports running handlers in dispatch threads for short duration request types (up to a few hundred nanoseconds), and worker threads for longer running requests. Which mode to use is specified when the request handler is registered. This is the only additional user input needed in eRPC.

### Scalable connection state
eRPCâ€™s choice to use packet I/O over RDMA avoids the circular buffer scalability bottleneck in RDMA. It can use constant space in the NIC by replacing NIC-managed connection state with CPU-managed connection state.

### Zero-copy transmission
The msgbuf layout ensures that the data region is contiguous (so that applications can use it as an opaque buffer) even when the buffer contains data for multiple packets. The first packetâ€™s data and header are also contiguous so that the NIC can fetch small messages with one DMA read. Headers for remaining packets are at the end, to allow for the contiguous data region in the middle.
eRPC provides zero copy reception for workloads under the common case of single packet requests and dispatch mode request handlers too.

### Sessions and flow control
Sessions support concurrent requests (8 by default) that can complete out-of-order. Use an array of slots to track RCP metadata for outstanding requests, and slots have an MTU-size pre-allocated msgbuf for use by request handlers that issue short responses. Session credits are used to implement packet-level flow control. Session credits also support end-to-end flow control to reduce switch queuing. Each session is given BDP/MTU credits, which ensures that each session can achieve line rate.

### Client-driven wire protocol
Client-driven protocols have fewer moving parts, with only the client needing to maintain wire protocol state. Single-packet RPCs (request and response require only a single packet) use the fewest packets possible. With multi-packet responses and a client-driven protocol the server canâ€™t immediately send response packets after the first one, so the client sends a request-for-response (RFR) packet.

### Congestion control
Three optimization brought the overhead of congestion control down from around 20% to 9%:
*    Bypassing Timely altogether is the RTT of a received packet on an uncongested session is less than a low threshold value.
*    Bypassing the rate limiter for uncongested sessions
*    Sampling timers once per RX or TX batch rather than once per packet for RTT measurement

These optimization works because datacenter networks are typically uncongested.

### Packet loss
eRPC keeps things simple by treating re-ordered packets as losses and dropping them (as do current RDMA NICs). When a client suspects a lost packet, it rolls back the requestâ€™s wire protocol state using a â€˜go-back-Nâ€™ mechanism. It reclaims credits and retransmits from the rollback point.

# Limitations
## Network Virtualization
* Open flow is very expensive and is a scaling bottleneck.
* The datacenters need to maintain additional hardware to support network virtualization and need to perform coordinations with the controller cluster. Upon failure of the coordination cluster, the whole datacenter would be impacted.

## 1RMA
* 1RMA's security protocol doesn't meaningfully change the behaviour of a host with root-level compromise; a root level attacker can impersonate the processes on the host and authenticate as one of the users.
* There is a significant overhead of the header data for every 4KB block generateed after segmentation.

## eRPC
* It is assumed that the datacenter is mostly congestion free, which may not be the case in smaller datacenters with relatively older hardware.

# Open Questions and Future Work
* Can the network virtualization be decentralized rather than depending on a separate cluster?
* Can the header overhead be reduced in 1RMA by building over the existing TCP/IP protocol?
